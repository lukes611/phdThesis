
\subsection{$G^2$o}
\label{Sec:G20}
The Graph SLAM algorithm sets relationships between camera poses and locations as nodes in a graph. Rather than defining camera pose estimations as truth, they are represented using a Gaussian in which the peak probability is the estimated value. Then using the visible landmarks (visible from multiple views) and the functions for the camera poses the joint graph is optimized for the improved pose estimates. Once this is complete, a 3D reconstruction map is produced. \\

Typically an occupancy grid or octree is used to represent the map \cite{Wurm10Octomap}. Further detail about the Graph SLAM method is outside the scope of this work, more detail can be found in the original paper on Graph SLAM \cite{Kummerle11G}. \\

\subsection{Bundle Adjustment}

Bundle Adjustment is another optimization technique used in SLAM and 3D reconstruction \cite{Lourakis09Sba}. In this context bundle refers to the light leaving each camera projected into world coordinates. This techniques refines both camera pose and visual reconstruction. It requires some estimates for camera pose, 3D world point positions and image point locations for each view. \\

Using these data and any non-linear optimization method (such as Gauss-Newton or the Levenbergâ€“Marquardt algorithm) the rigid motion based camera movement and the 3D world-coordinates of the points are optimized. \\

In the literature, Fioraio \cite{Fioraio11Realtime} presented a system which uses bundle adjustment to align rgb-d data. It has also been used in conjunction with feature matching to produce sparse 3D reconstructions \cite{Klein07Parallel,Agarwal09Building}.

[END BUNDLE ADJUSTMENT]



ICP:

[FM Section]
Whelan et al \cite{Whelan12Kintinuous} extended Kinectfusion allowing it to map larger areas. They compared ICP with FOVIS and found that FOVIS contributed less drift but lack-luster models compared with ICP. Whelan et al also proposed a technique for incrementally creating a triangle mesh as the reconstruction was built. This method uses multi-threadeding, and uses pose graph optimization (see section \ref{Sec:G20}).

[FOVIS in FM section]
Another technique which is based on feature matching is the FOVIS (Fast Odometry from Vision) technique \cite{Huang17Visual}. In FOVIS works in 6 stages. In the first two stages, the RGB-D image data is collected and put through a low-pass filter, Gaussian scale space is computed and then FAST features \cite{Rosten06Machine,Rosten05Fusing} are computed. In the next stage, initial camera rotation estimation is computed by minimizing the sum of squared errors between down sampled versions of consecutive image frames. In the 4th stage, feature matching is performed, using the SAD (sum of absolute differences) metric and sub-pixel resolution. In the 5th stage, outliers are removed using a feature match graph and a greedy algorithm to find maximal cliques \cite{Hirschmuller02Fast,Howard08Real}. \\

In the final stage, the full camera pose is estimated by first computing it using Horns method \cite{Horn87Closed} This technique minimizes an error function in computing the camera rotation and translation. The FOVIS method then optimizes this by minimizing the re-projection error using a non-linear least squares method.

2.  
Whelan et al improved \cite{Whelan13Robust} upon their previous method \cite{Whelan12Kintinous} by integrating FOVIS and ICP together on the GPU, and adding an advanced colour fusion model to the technique. The main improvement lies in switching between FOVIS and ICP whenever the error using FOVIS is too high, ICP is used. They also integrate using a global model rather than a local one. 


icp slips on planar environments or environments with little texture \cite{Whelan13Robust}

3,4
\cite{Stuckler12Robust} - use an efficient and robust version of ICP


\subsection{RGB-D Sensor Feature Based Systems}




\subsection{Non-Feature Based Methods}


Weikersdorfer et al \cite{Weikersdorfer14Event} presented a novel sensor system named D-eDVS along with an event based SLAM algorithm. The D-eDVS sensor combines depth and event driven contrast detection. Rather than using features, it uses all detected data for registration. - aligns event transmitting device with Asus-xtion pro rgb-d camera. then using the transform is computed by solving a least squares algorithm

Kerl et al \cite{Kerl13Dense} proposed a dense RGB-D SLAM system which uses a probabilistic camera parameter estimation procedure, it formulates the projection error as an image warping function and solves it using taylors expansion optimization. It uses the entire image rather than features to perform SLAM.


\subsection{Summary}
As is evident from the current literature, SLAM typically relies on feature matching and RANSAC. However, these approaches fail when there are too few features, when feature confusion occurs or, when features are non-stationary due to object motion. As the extent of random feature displacement becomes more global the effectiveness of these approaches diminishes. Feature matching also dominates in image registration. However, Fourier based methods have been shown to work well under larger rotations and scales \cite{Gonzalez11Improving} whilst being closed form, insensitive to object motion and scaling naturally to GPU implementations. Accordingly, we propose a novel, closed form Fourier based SLAM method.

Simultaneous localization and mapping (SLAM) has applications in many fields including: robotics, business, architecture and engineering, and science. Its goal is to generate a map (2D birds-eye view, or 3D) of an environment captured by camera and/or other means. In this work we focus on monocular systems, or systems which generate location and mapping data using information generated by a single basic video camera. To this end, current methods rely on the computation of the fundamental and essential matrices. These feature matching techniques fail in cases where features are not stable or where feature confusion occurs. 

It has been shown [1] that using volume registration to compute dense 3D maps is not only independent of feature matching, but it is a closed form solution and is robust to noise and object motion. However, this method requires RGB-D video input provided by special hardware. In this paper we present preliminary results in applying volume registration to generate dense 3D maps from monocular video data. To achieve this, disparity maps are generated between video frames. This data is then used as input for the RGB-D volume registration method.
